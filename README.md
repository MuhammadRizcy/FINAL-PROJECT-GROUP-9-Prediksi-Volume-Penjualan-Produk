# ğŸ§  Product Sales Volume Prediction â€“ GDGOC 2025

## ğŸ“˜ Overview
This project was developed as part of the **Google Developer Group Open Competition (GDGOC) 2025**, focusing on predicting the **sales volume of a product** using historical data.  

The goal of this project is to build a machine learning model capable of estimating future product sales volumes based on features such as product type, price, marketing spend, store location, and seasonal trends.  

This project demonstrates a complete end-to-end data science workflow â€” from data cleaning, exploratory data analysis, feature engineering, and model training, to evaluation and interpretation â€” as a part of Group 9â€™s final GDGOC project.

---

## ğŸ“Š Dataset
- **Source:** Provided during GDGOC 2025 competition  
- **Total Samples:** ~10,000 records  
- **Main Features:**
  - `product_name` â€“ type of product  
  - `category` â€“ product category  
  - `price` â€“ selling price  
  - `marketing_spend` â€“ amount spent on promotion  
  - `store_location` â€“ sales outlet region  
  - `month`, `year` â€“ temporal features  
  - `sales_volume` â€“ target variable (number of products sold)  

---

## âš™ï¸ Workflow

### 1. Exploratory Data Analysis (EDA)
- Analyzed distribution of product sales and feature relationships.  
- Detected correlations between marketing spend, price, and total sales.  
- Visualized seasonal sales trends across different months and product categories.

### 2. Data Preprocessing
- **Handling Missing Values:** Imputed missing values using `SimpleImputer`.  
- **Encoding:** Converted categorical columns using one-hot encoding.  
- **Scaling:** Standardized numerical features using `StandardScaler`.  
- **Feature Selection:** Removed irrelevant and low-variance columns.

### 3. Feature Engineering
- Created new features such as:
  - Average monthly sales per product
  - Price-to-sales ratio
  - Seasonal index features  
- Combined time-based variables to capture trends and seasonality.

### 4. Model Training
- Trained several regression models including:
  - `LinearRegression`
  - `RandomForestRegressor`
  - `XGradientBoostingRegressor`
- Performed hyperparameter tuning using cross-validation to select the best-performing model.

### 5. Model Evaluation
- Evaluation Metrics: **Mean Absolute Error (MAE)** and **RÂ² Score**  
- Best model achieved:
  - **MAE:** â‰ˆ : 0.3206 
  - **RÂ² Score:** â‰ˆ 0.82  
- Results showed the XGradientBoostingRegressor model produced the most stable predictions and lowest error.
